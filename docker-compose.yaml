version : '2'

services:

  # -------- API GATEWAY --------
  nginx:
    restart: always
    container_name: nginx
    depends_on:
      - chatting-server-1
      - chatting-server-2
    build:
      context: ./nginx
      dockerfile: Dockerfile
    ports:
      - "8080:80"

  # -------- KAFKA --------
  zookeeper:
    image: confluentinc/cp-zookeeper:7.2.1
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
  kafka1:
    image: confluentinc/cp-kafka:7.2.1
    container_name: kafka1
    ports:
      - "8097:8097"
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: EXTERNAL:PLAINTEXT,INTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: EXTERNAL://localhost:8097,INTERNAL://kafka1:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
  kafka2:
    image: confluentinc/cp-kafka:7.2.1
    container_name: kafka2
    ports:
      - "8098:8098"
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: EXTERNAL:PLAINTEXT,INTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: EXTERNAL://localhost:8098,INTERNAL://kafka2:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
  kafka3:
    image: confluentinc/cp-kafka:7.2.1
    container_name: kafka3
    ports:
      - "8099:8099"
    depends_on:
      - zookeeper
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: EXTERNAL://localhost:8099,INTERNAL://kafka3:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: EXTERNAL:PLAINTEXT,INTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL

  # -------- Chatting Server --------
  chatting-db-1:
    container_name: chatting-db-1
    image: postgres:12-alpine
    environment:
      - POSTGRES_PASSWORD=password
      - POSTGRES_USER=postgres
      - POSTGRES_DB=chat
    expose:
      - "5433" # Publishes 5433 to other containers but NOT to host machine
    ports:
      - "5433:5433"
    volumes:
      - ./backups:/home/backups
    command: -c wal_level=logical -p 5433

  chatting-server-1:
    container_name: chatting-server-1
    build: ./spring-chatting-backend-server
    ports:
      - "8082:8082"
    environment:
      - SPRING_DATASOURCE_URL=jdbc:postgresql://chatting-db-1:5433/chat
      - SPRING_DATASOURCE_USERNAME=postgres
      - SPRING_DATASOURCE_PASSWORD=password
      - SPRING_JPA_HIBERNATE_DDL_AUTO=update
      - SERVER_PORT=8082
      - KAFKA_BOOTSTRAP=kafka1:9092,kafka2:9092,kafka3:9092 # 내부포트
    depends_on:
      - kafka1
      - kafka2
      - kafka3
      - chatting-db-1
    restart: always
  chatting-db-2:
    container_name: chatting-db-2
    image: postgres:12-alpine
    environment:
      - POSTGRES_PASSWORD=password
      - POSTGRES_USER=postgres
      - POSTGRES_DB=chat
    expose:
      - "5434" # Publishes 5433 to other containers but NOT to host machine
    ports:
      - "5434:5434"
    volumes:
      - ./backups:/home/backups
    command: -c wal_level=logical -p 5434

  chatting-server-2:
    container_name: chatting-server-2
    build: ./spring-chatting-backend-server
    ports:
      - "8084:8084"
    environment:
      - SPRING_DATASOURCE_URL=jdbc:postgresql://chatting-db-2:5434/chat
      - SPRING_DATASOURCE_USERNAME=postgres
      - SPRING_DATASOURCE_PASSWORD=password
      - SPRING_JPA_HIBERNATE_DDL_AUTO=update
      - SERVER_PORT=8084
      - KAFKA_BOOTSTRAP=kafka1:9092,kafka2:9092,kafka3:9092 # 내부포트
    depends_on:
      - kafka1
      - kafka2
      - kafka3
      - chatting-db-2
    restart: always

  # -------- Authentication Server --------
  auth-db:
    container_name: auth-db
    image: postgres:12-alpine
    environment:
      - POSTGRES_PASSWORD=password
      - POSTGRES_USER=postgres
      - POSTGRES_DB=auth
    expose:
      - "5435" # Publishes 5433 to other containers but NOT to host machine
    ports:
      - "5435:5435"
    volumes:
      - ./backups:/home/backups
    command: -p 5435
  auth-server:
    container_name: auth-server
    build: ./spring-auth-backend-server
    ports:
      - "8085:8085"
    environment:
      - SERVER_PORT=8085
      - SPRING_DATASOURCE_URL=jdbc:postgresql://auth-db:5435/auth
      - SPRING_DATASOURCE_USERNAME=postgres
      - SPRING_DATASOURCE_PASSWORD=password
      - SPRING_JPA_HIBERNATE_DDL_AUTO=update
    depends_on:
      - auth-db
    restart: always

  # -------- kafdrop for visualization --------
  kafdrop:
    image: obsidiandynamics/kafdrop
    restart: "no"
    environment:
      KAFKA_BROKERCONNECT: "kafka1:9092,kafka2:9092,kafka3:9092"
      JVM_OPTS: "-Xms16M -Xmx512M -Xss180K -XX:-TieredCompilation -XX:+UseStringDeduplication -noverify"
    ports:
      - 9000:9000
    depends_on:
      - kafka1
      - kafka2

  # -------- postgres -> kafka source connector --------
  kafka-source-connector:
    image: debezium/connect:1.9
    container_name: postgres-kafka-source-connector
    ports:
      - 8083:8083
    environment:
      CONFIG_STORAGE_TOPIC: __pg.source.config.storage
      OFFSET_STORAGE_TOPIC: __pg.source.offset.storage
      STATUS_STORAGE_TOPIC: __pg.source.status.storage
      BOOTSTRAP_SERVERS: kafka1:9092,kafka2:9092,kafka3:9092
    depends_on:
      - kafka1
      - kafka2
      - kafka3
      - zookeeper
      - chatting-db-1
      - chatting-db-2

  # rest api 를 통한 kafka source connector 등록

#   POST  http://localhost:8083/connectors
#   {
#     "name": "inventory-connector",
#     "config": {
#       "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
#       "database.hostname": "postgres",
#       "database.port": "5433",
#       "database.user": "postgres",
#       "database.password": "password",
#       "database.dbname" : "chat",
#       "database.server.name": "dbserver1",
#       "table.include.list": "chat.user_table"
#     }
#   }


  # -------- kafka -> postgres sink connector --------
  # -------- postgres -> kafka source connector --------
  kafka-connect:
    image: confluentinc/cp-kafka-connect
    container_name: kafka-connect
    ports:
      - "8091:8083"
    depends_on:
      - kafka1
      - kafka2
      - kafka3
      - chatting-db-1
      - chatting-db-2
      - auth-db
      - zookeeper
    environment:
      LOG_LEVEL: DEBUG
      CONNECT_BOOTSTRAP_SERVERS: "kafka1:9092,kafka2:9092,kafka3:9092"
      CONNECT_GROUP_ID: kafka-connect
      CONNECT_CONFIG_STORAGE_TOPIC: __pg.sink.config.storage
      CONNECT_STATUS_STORAGE_TOPIC: __pg.sink.status.storage
      CONNECT_OFFSET_STORAGE_TOPIC: __pg.sink.offset.storage
      CONNECT_VALUE_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_KEY_CONVERTER: io.confluent.connect.avro.AvroConverter
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_REST_PORT: 8091
      CONNECT_REST_ADVERTISED_HOST_NAME: "kafka-connect"
      CONNECT_SCHEMAS_ENABLE: "false"
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_ZOOKEEPER_CONNECT: zookeeper:2181
      #  ---------------
      CONNECT_PLUGIN_PATH: /usr/share/java,/usr/share/confluent-hub-components,/data/connect-jars
    # If you want to use the Confluent Hub installer to d/l component, but make them available
    # when running this offline, spin up the stack once and then run :
    #   docker cp kafka-connect:/usr/share/confluent-hub-components ./data/connect-jars
    volumes:
      - ./connectors:/etc/kafka-connect/jars/
    # In the command section, $ are replaced with $$ to avoid the error 'Invalid interpolation format for "command" option'
    command:
      - bash
      - -c
      - |
        echo "Installing Connector"
        confluent-hub install --no-prompt confluentinc/kafka-connect-jdbc:10.6.1
        confluent-hub install --no-prompt confluentinc/connect-transforms
        #
        echo "Launching Kafka Connect worker"
        /etc/confluent/docker/run
